# -*- coding: utf-8 -*-
"""Proiect_TIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQWi-OkBC1RgfqRUSz672VrpJgWT1jm1
"""

# Import biblioteci necesare
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, classification_report
from google.colab import drive
import numpy as np
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from lightgbm import LGBMRegressor
import matplotlib.pyplot as plt
import seaborn as sns



# drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/Colab Notebooks/Set_Antrenare_TIA'
dataset = pd.read_csv(file_path)

# Încarcă setul de date curățat
dataset = pd.read_csv(file_path)  # Înlocuiește cu calea ta
print(dataset)

# Transformă 'Position' în variabile one-hot
# label_encoder = LabelEncoder()
# position_encoder = LabelEncoder()
# dataset['Position_Encoded'] = label_encoder.fit_transform(dataset['Position'])

# positions = ['Defender', 'Attacker', 'Midfielder']
# position_encoder.fit(positions)

features = dataset[['Position','Ability','Age','Reputation']]

target = dataset['Transfer Value']

features = pd.get_dummies(features, columns=['Position'])
scaler = StandardScaler()
numeric_columns = ['Ability', 'Age', 'Reputation']
features[numeric_columns] = scaler.fit_transform(features[numeric_columns])

# Împărțirea setului de date în seturi de antrenament și test
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.22, random_state=42)


# y_pred = best_model.predict(X_test)
# mse = mean_squared_error(y_test, y_pred)
# r2 = r2_score(y_test, y_pred)

# print(f"Mean Squared Error (MSE): {mse}")
# print(f"R-squared (R²): {r2}")

# model = XGBRegressor(
#     n_estimators=150,
#     learning_rate=0.05,
#     max_depth=4,
#     subsample=0.8,
#     colsample_bytree=0.8,
#     random_state=42,
#     objective='reg:squarederror'
# )

# # Model CatBoost
# model = CatBoostRegressor(iterations=200, learning_rate=0.1, depth=6, verbose=0)

# Antrenarea modelului
# model.fit(X_train, y_train)

# # Testarea modelului
# y_pred = model.predict(X_test)
# results = pd.DataFrame({
#     'Transfer Value': y_test,
#     'Predicted Value': y_pred
# })
# print("Valori reale vs. prezise:")
# print(results)

# Evaluarea performanței modelului
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")

# Determină media valorilor reale pentru a le împărți în două categorii: sub sau peste medie
median_value = y_test.median()

# Convertește valorile reale și predicțiile în categorii binare
y_test_binary = (y_test >= median_value).astype(int)
y_pred_binary = (y_pred >= median_value).astype(int)

# Calculează matricea de confuzie
cm = confusion_matrix(y_test_binary, y_pred_binary)

# Afișează matricea de confuzie folosind seaborn pentru un aspect mai frumos
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matricea de Confuzie')
plt.ylabel('Valori reale')
plt.xlabel('Predicții')
plt.show()

# Etichetele pentru axele x și y (0 - sub medie, 1 - peste medie)
plt.yticks([0.5, 1.5], ['Sub Medie', 'Peste Medie'])
plt.xticks([0.5, 1.5], ['Sub Medie', 'Peste Medie'])
plt.show()

print({median_value})
print(f"Matricea de confuzie:\n{cm}")

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Gradient Boosting Regressor
# gb_model = GradientBoostingRegressor(
#     n_estimators=200,
#     learning_rate=0.05,
#     max_depth=3,
#     random_state=42
# )

# # Antrenarea și evaluarea modelului GB
# gb_model.fit(X_train_scaled, y_train)
# gb_predictions = gb_model.predict(X_test_scaled)
# gb_mse = mean_squared_error(y_test, gb_predictions)
# gb_r2 = r2_score(y_test, gb_predictions)
# print(f"Gradient Boosting - MSE: {gb_mse}, R²: {gb_r2}")

# LightGBM Regressor
lgbm_model = LGBMRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)

# Antrenarea și evaluarea modelului LGBM
lgbm_model.fit(X_train_scaled, y_train)
lgbm_predictions = lgbm_model.predict(X_test_scaled)
lgbm_mse = mean_squared_error(y_test, lgbm_predictions)
lgbm_r2 = r2_score(y_test, lgbm_predictions)
print(f"LightGBM - MSE: {lgbm_mse}, R²: {lgbm_r2}")

results = pd.DataFrame({
    'Transfer Value': y_test,
    'Predicted Value': lgbm_predictions
})
print("Valori reale vs. prezise:")
print(results)
print(X_test_scaled)

from google.colab import drive
drive.mount('/content/drive')

from catboost import CatBoostRegressor

# Set de date de testare (10 exemple)
test_data = pd.DataFrame({
    'Player': ['Mazraoui', 'Maddison', 'Calvert-Lewin', 'Toney', 'Neto',
               'Willock', 'Fekir', 'Diomande', 'Tarkowski', 'Amadou Onana',
               'Fofana', 'Thuram', 'Kehrer', 'Gimenez', 'Werner'],
    'Position': ['Defender', 'Midfielder', 'Attacker', 'Attacker', 'Attacker',
                 'Midfielder', 'Midfielder', 'Defender', 'Defender', 'Midfielder',
                 'Defender', 'Attacker', 'Defender', 'Defender', 'Attacker'],
    'Ability': [149, 159, 150, 150, 138, 145, 148, 142, 140, 137, 151, 157, 135, 149, 143],
    'Age': [25, 26, 26, 27, 23, 23, 29, 19, 30, 21, 22, 25, 26, 28, 27],
    'Reputation': [130, 135, 125, 130, 122, 115, 133, 122, 120, 115, 120, 137, 110, 140, 125],
    'Transfer Value': [29, 75, 55, 48, 70, 50, 30, 45, 30, 66, 80, 70, 25, 40, 25]
})

train_features = dataset[['Position', 'Ability', 'Age', 'Reputation']]
train_features = pd.get_dummies(train_features, columns=['Position'])

train_features[numeric_columns] = scaler.fit_transform(train_features[numeric_columns])

test_features = test_data[['Position','Ability', 'Age', 'Reputation']]
test_features = pd.get_dummies(test_features, columns=['Position'])

test_features = test_features.reindex(columns=train_features.columns, fill_value=0)

test_features[numeric_columns] = scaler.transform(test_features[numeric_columns])

test_data['Predicted Value'] = lgbm_model.predict(test_features)

mse = mean_squared_error(test_data['Transfer Value'], test_data['Predicted Value'])
r2 = r2_score(test_data['Transfer Value'], test_data['Predicted Value'])

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R²): {r2:.2f}")
print("\nValori reale vs. prezise:")
print(test_data[['Player', 'Transfer Value', 'Predicted Value']])


num_bins = 3
bin_intervals = [20,40,60,80]
test_data['True Bin'] = pd.cut(test_data['Transfer Value'], bins=bin_intervals, labels=False, include_lowest=True)
test_data['Predicted Bin'] = pd.cut(test_data['Predicted Value'], bins=bin_intervals, labels=False, include_lowest=True)
bin_labels = [f"[{bin_intervals[i]}, {bin_intervals[i+1]})" for i in range(len(bin_intervals) - 1)]

# Matricea de confuzie
conf_matrix = confusion_matrix(test_data['True Bin'], test_data['Predicted Bin'], labels=range(num_bins))

# Afișare matrice
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=bin_labels, yticklabels=bin_labels)
plt.title("Matrice de Confuzie")
plt.xlabel("Predicted Bin")
plt.ylabel("True Bin")
plt.show()

"""regresie polinomiala"""